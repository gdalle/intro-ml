import marimo

__generated_with = "0.10.9"
app = marimo.App(
    width="medium",
    app_title="ML in 1h - theory",
    layout_file="layouts/theory.slides.json",
)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""# Machine Learning in 1h - theory""")
    return


@app.cell(hide_code=True)
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md("""## Key concepts""")
    return


@app.cell(hide_code=True)
def _(mo):
    tabs = mo.ui.tabs(
        {
            "Thinking humanly": mo.md("""
            > ‚ÄúThe exciting new effort to make computers think... machines with minds, in the full and literal sense.‚Äù (Haugeland, 1985)

            > ‚ÄúThe study of mental faculties through the use of computational models.‚Äù (Charniak and McDermott, 1985)  

            > ‚Äú[The automation of] activities that we associate with human thinking, activities such as decision-making, problem solving, learning...‚Äù (Bellman, 1978)
            """),
            "Thinking rationally": mo.md("""
            > ‚ÄúThe study of mental faculties through the use of computational models.‚Äù (Charniak and McDermott, 1985)

            > ‚ÄúThe study of the computations that make it possible to perceive, reason, and act.‚Äù (Winston, 1992)
            """),
            "Acting humanly": mo.md("""
            > ‚ÄúThe art of creating machines that perform functions that require intelligence when performed by people.‚Äù (Kurzweil, 1990)

            > ‚ÄúThe study of how to make computers do things at which, at the moment, people are better.‚Äù (Rich and Knight, 1991)
            """),
            "Acting rationally": mo.md("""
            > ‚ÄúComputational Intelligence is the study of the design of intelligent agents.‚Äù (Poole et al., 1998)

            > ‚ÄúAI... is concerned with intelligent behavior in artifacts.‚Äù (Nilsson, 1998)
            """),
        }
    )

    mo.md(f"""
    ### Artificial intelligence

    {tabs}

    [^source]: Definitions from `AIMA`
    """)
    return (tabs,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        ### Machine learning

        > "Machine learning is a subfield of computer science that is concerned with building algorithms which, to be useful, rely on a collection of examples of some phenomenon. These examples can come from nature, be handcrafted by humans or generated by another algorithm." (Burkov, 2019)
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    examples = mo.accordion(
        {
         "Traveling Salesman Problem":   mo.md(
                f"""
                <https://www.math.uwaterloo.ca/tsp/>

                {mo.image("https://www.math.uwaterloo.ca/tsp/uk/img/uk49_main_all.jpg")}
                """
            ),
            "Traffic prediction with advanced Graph Neural Networks": mo.md(
                f"""
                <https://deepmind.google/discover/blog/traffic-prediction-with-advanced-graph-neural-networks/>

                {mo.image("https://lh3.googleusercontent.com/uD-xEFB9SggZTkd_BN1566SHdO1xVe4RiALacVVE377xrxM_Mlg68CPzUjQmq_quBmAJo4Rl5_MQYO81CX1oYSw_9iT51eIXaIgonOvEIqWlVHezN3E=w616-rw")}
                """
            ),
            "World scale inverse reinforcement learning in Google Maps": mo.md(
                f"""
                <https://research.google/blog/world-scale-inverse-reinforcement-learning-in-google-maps/>

                {mo.image("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHUdaeXJURVEV2f--mytRfbx5k3yftkMKBpuu43GKhyMaAa-fJC3O4QfTxZjz3hkGjUsCBiO6LLcYRsGeyrpOgIUzDDPW1lOwlr47YXKSLUwJuXxMELxRV9SKcr4BDoy5_2HzcFZX-Wb7m3PcQhWvx1efvG5gkXG_HfrhqrqfZ_xrNyBmSado-Wf_si3wH/s16000/image3.png")}
                """
            ),
        }
    )

    mo.md(
        f"""
        ### From AI to ML

        {examples}
        """
    )
    return (examples,)


@app.cell(hide_code=True)
def _(mo):
    learning_types = mo.accordion(
        {
            "Supervised learning": mo.md("""
            |data type|goal|
            |---|---|
            |features and labels|predict outcome|
            """),
            "Unsupervised learning": mo.md("""
            |data type|goal|
            |---|---|
            |features alone|find structure|
            """),
            "Reinforcement learning": mo.md("""
            |data type|goal|
            |---|---|
            |interactive environment|define action policy|
            """),
        }
    )

    mo.md(
        f"""
        ### Learning paradigms

        {learning_types}
        """
    )
    return (learning_types,)


@app.cell(hide_code=True)
def _(mo):
    problems = mo.ui.tabs(
        {
            "Supervised": mo.accordion(
                {
                    "Regression": "Predict numeric label",
                    "Classification": "Predict categorical label",
                }
            ),
            "Unsupervised": mo.accordion(
                {
                    "Clustering": "Define groups of similar samples",
                    "Dimensionality reduction": "Represent data in a simpler space",
                }
            ),
        }
    )

    mo.md(
        f"""
        ### Problem types

        {problems}
        """
    )
    return (problems,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        f"""
        ### Choosing an algorithm

        {mo.image("https://scikit-learn.org/stable/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg")}
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md("""## Linear models""")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        ### Setting

        - Goal: approximate an unknown function $f: \mathbf{x} \longmapsto y$
        - Features $\mathbf{x} \in \mathbb{R}^d$, label $y \in \mathbb{R}$ (regression) or $y \in \{1, \dots, C\}$ (classification)
        - Parametric family of approximations $\{f_\theta : \theta \in \Theta\}$ to choose from
        - Dataset of samples $(\mathbf{x}^{(1)}, y^{(1)}), ..., (\mathbf{x}^{(n)}, y^{(n)})$ to train on
        - Often represented as "tidy data": matrix of features $\mathbf{X}$ and vector of labels $\mathbf{y}$, with one row per sample
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        ### Linear regression

        Hypothesis: the numeric label is a linear combination of the features.

        $$\widehat{y} = f_\theta(\mathbf{x}) = \sum_{j=1}^d \theta_j x_j$$

        The parameter vector $\theta$ contains the weights of each feature.
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        ### Logistic regression

            Actually a classification model.

            Hypothesis: the frontier between both categories is a linear combination of the features.

            $$\widehat{y} = f_\theta(\mathbf{x}) = \begin{cases} 1 & \text{if}~ \sum_{j=1}^d \theta_j x_j > 0 \\ 0 & \text{otherwise} \end{cases}$$
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        ### Probabilistic perspective

        Hypothesis: the label is generated randomly from the features

        ||Linear regression|Logistic regression|
        |---|---|---|
        | Label distribution | Gaussian $y \sim \mathcal{N}(\mu, \sigma^2)$ | Bernoulli $y \sim \mathcal{B}(p)$ |
        | Moment parameter | $\mu = \sum_j \theta_j x_j$, $\sigma$ fixed | $p = \mathrm{sigmoid}(\sum_j \theta_j x_j)$
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        ### Likelihood and loss

        The probabilistic perspective gives a natural way to select the parameter $\theta$: Maximum Likelihood Estimation (MLE)

        For linear regression, the likelihood is given by

        $$\mathbb{P}_\theta(y | \mathbb{x}) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(y-\theta^\top \mathbf{x})^2}{2\sigma^2}\right)$$

        The MLE is the solution to the Ordinary Least Squares problem on the training set:

        $$\max_\theta ~ \mathbb{P}_\theta(y^{(1:n)} | \mathbf{x}^{(1:n)}) = \min_\theta ~ \sum_{i=1}^n (y^{(i)} - \theta^\top \mathbf{x}^{(i)})^2 $$

        This function is also called a loss $\mathcal{L}(\theta)$.
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md("""## Main challenges""")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        ### Sources of error

        - Approximation error: the right function is not in our parametric family
        - Generalization error: the training set has not prepared us for every possibility
        - Estimation error: we failed to compute the true minimizer of the loss

        Plus all the sources of error linked to the data and pipeline
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    overfitting_img = mo.image(
        "https://scikit-learn.org/stable/_images/sphx_glr_plot_underfitting_overfitting_001.png",
        caption="Source: `scikit-learn` documentation",
    )

    mo.md(
        f"""
        ### Underfitting / overfitting

        | model complexity | phenomenon | consequence |
        |---|---|---|
        | too low | underfitting | bad performance during training |
        | too high | overfitting | bad performance during testing |

        {overfitting_img}
        """
    )
    return (overfitting_img,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""## Full pipeline""")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
        ### Preprocessing

        - Read data from source(s)
        - Encode in the right format
        - Clean up erroneous entries
        - Handle missing values
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
        ### Training

        - Estimate best parameters based on the training set
        - Minimizing a loss function with gradient descent
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
        ### Evaluation

        - Done on a separate test set to prevent contamination
        - Relies on a metric chosen in advance
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
        ### Model selection

        - Some model hyperparameters are chosen arbitrarily and not the result of training.
        - Those must be chosen according to the performance on yet another set, the validation set.
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md("""## Software tools""")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        ### Languages

        [Python](https://www.python.org/) is the language of choice for machine learning.

        There are other options with more specific skill sets:

        - [R](https://www.r-project.org/) for traditional statistical analysis
        - [Julia](https://julialang.org/) for [scientific machine learning](https://sciml.ai/) (ask me if you're curious)
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        f"""
        ### `scikit-learn`

        - French product ü•ê initially developed at Inria
        - Suite of basic machine learning tools
        - No deep learning (usually not necessary)

        {mo.image("https://github.com/scikit-learn/scikit-learn/blob/main/doc/logos/scikit-learn-logo.png?raw=true")}
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        ### Other libraries

        You already know some of these:

        - [`pandas`](https://pandas.pydata.org/) / [`polars`](https://docs.pola.rs/) for data wrangling
        - [`numpy`](https://numpy.org/) / [`scipy`](https://scipy.org/) for numerical and scientific computing
        - [`matplotlib`](https://matplotlib.org/) / [`seaborn`](https://seaborn.pydata.org/) for visualization
        - [`networkx`](https://networkx.org/) / [`rustworkx`](https://www.rustworkx.org/) for network science
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md("""## Going further""")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
        ### Courses

        * [The Illustrated Machine Learning website](https://illustrated-machine-learning.github.io/#/)
        * [Scikit-learn MOOC by its creators](https://inria.github.io/scikit-learn-mooc/)
        * [Google Machine Learning Education](https://developers.google.com/machine-learning)
        * [Coursera Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
        ### Books

        - `HPML`: [The Hundred-page Machine Learning Book](https://themlbook.com/) (Burkov, 2019)
        - `MLE`: [Machine Learning Engineering](http://mlebook.com/) (Burkov, 2020)
        - `MLPS`: [Machine Learning with PyTorch and Scikit-Learn](https://sebastianraschka.com/blog/2022/ml-pytorch-book.html) (Raschka, 2022)
        - `AIMA`: [Artificial Intelligence: A Modern Approach](https://aima.cs.berkeley.edu/) (Russell and Norvig, 2021)
        """
    )
    return


if __name__ == "__main__":
    app.run()
